#+TITLE: Supervised and Reinforcement Learning of Neural Agent Controllers
#+AUTHOR: Diego Vicente Martín 
#+EMAIL: diegovi@stud.ntnu.no
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [8pt]
#+LATEX_HEADER: \usepackage[margin=1cm]{geometry}
#+LATEX_HEADER: \pagenumbering{gobble}
#+LATEX_HEADER: \usepackage{wrapfig}
#+LANGUAGE: en
#+OPTIONS: toc:nil date:nil H:1 

* Flatland & Baseline Agent

** *Provide a brief overview of your implementation. Include a screenshot of your Flatland visualization.* 

The implementation is done in Python 3, and all the important source code files
are present in the ~src/~ folder. In there, we can find several different
files: 

- ~flatland.py~: In which we can find the ~Flatland~ class, which is in charge
  of the environment representation. This class also contains the necessary
  methods to query the board and look around in it.
- ~agents.py~: In which we can find the main agent (~Agent~), which implements
  the common tasks for each of the agents; as well as the required agents for
  the assignment. We can also find the ~Direction~ class, used to represent
  directions in the board.
- ~window.py~: which includes the ~Simulation~ class, that creates a visual
  representation of the last execution of an agent using ~pygame~.
- ~run.py~: prepares a demo execution to show the different agents.

#+CAPTION: Flatland environment and weights representation, with an input-output
#+ATTR_LATEX: :height 4cm
[[./img/input.png]]

** *Describe how your baseline agent decides whether to move left, forward or right.* 

The baseline agent (~GreedyAgent~) follows a simple policy, in which he first
tries to go for a cell with food, if not possible one empty, and if there is no
other choice he goes to a cell with poison. To make it perform better, if there
are several cells with the target content (i.e food front and left), the agent
always chooses to go front, to prevent loops due to empty cells.

* Supervised Learning of the Neural Agent

** *Include a few lines of code from the part of your program where the delta values are calculated and explain how this code implements Equation 3.*

#+BEGIN_SRC python
# Update each of the weights
i = output_values.index(max_out)
for j in range(len(self.neurons)):
    input_n = self.neurons[j]
    output_n = self.outputs[i][0]
    delta = correct - (math.exp(output_n - max_out / sum_exp))
    self.weights[(i, j)] += self.learning_rate * input_n * delta
#+END_SRC

In the snippet above, part of ~SupervisedAgent~, we can see how we the delta
rule is implemented to take into account if the policy action was succesfully
chosen or not.

* Reinforcement Learning of the Neural Agent

** *Include a few lines of code from the part of your program where the delta values ($\delta_{i}$) are calculated and explain how this code implements Equation 5.*

#+BEGIN_SRC python
i = self._prev_out
for j in range(len(self.neurons)):
    input_n = self._prev_neurons[j]
    delta = self._r + self.discount * max_q - self._prev_q
    self.weights[(i, j)] += self.learning_rate * input_n * delta
#+END_SRC

In this snippet, part of ~ReinforcementAgent~, we can see how the update of
weights is implemented using the attributes stored by the agent about the
previous turn. That means that the agent updates with a delay of one turn, and
this has to be taken into account when the run ends (specially when running
into walls). This is solved by forcing the last update of the agent. 

* Analysis
  
** *List all of the weights in the neural network. Make sure this is readable, e.g. by grouping the weights by output neuron and input sensor. You may also visualize the network in a graph (optional).*

As we can see in Figure 1, the weights of the network are visible when using
the graphical representation the whole time. The color of each weight indicates
how negative (red) or how positive (green) the weight is. In case a higher
detail is needed, pressing ~w~ in the visual simulation will print in the
terminal the weights dictionary in a readable way.

** *Select a few (at least 5) input situations and describe how the network responds (output values and which action is selected). Account for how the network generates its behavior (e.g. any structure you might find in the weights, etc.).*

This can be done by checking the neuron representation live in the graphical
execution. The neurons that are triggered appear marked with a blue dot, so the
weights connecting on neurons are the highest weights activated in each
moment. Some example situations are:
- Food in front, food right, food left: the agent chooses front, has learnt
  that can run into loops.
- Empty in front, empty right, food left: the agent will almost always choose
  the food since it is positive reinforcement.
- Poison in front, food left, food right: the agent can choose right or left,
  depending on its recent movements. Knows that poison is bad.
- Poison in front, poison left, empty right: chooses right to dodge poison.
- poison in front, wall left, poison right: will choose on of the two poisons,
  since it has learnt that poison is bad reinforcement but wall is far worse.

** *How did your agents’ results progress from task 1 (baseline agent) to task 2 (supervised), task 3 (reinforcement) and task 4 (reinforcement with extended sensor range)? Was this as expected?*

#+CAPTION: Average scores obtained by each of the agents in 50 rounds of training
#+ATTR_LATEX: :height 4cm
[[./img/agents.png]]

The baseline agent performs in a deterministic way, so it is easy for it to
perform on an average of 20. The supervised agent, using the adequate learning
rate, is the fastest learning but its performance is rarely better than the
baseline one, as expected since it learnt from the baseline. On the other hand,
even though it sometimes struggles to learn properly the task (due to
inconvenient worlds in the beginning of the learning phase), the reinforcement
agent is able to perform slightly better than the baseline one
sometimes. Finally, the best one performing is the enhanced one, since it is
able to interpret more information from the environment, which also makes it
the slowest one to learn.

